{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4018927b-6e79-4685-9b64-5e87194a042c",
   "metadata": {},
   "source": [
    "# Entrenamiento de la SRCNN\n",
    "Presentado por:\n",
    "- Luis Torres\n",
    "- Alexis Madrigal\n",
    "- Alejandro Tevera\n",
    "\n",
    "Nuestra SRCNN constará de 3 capas convolucionales, donde la primer capa tendrá 128 entradas, es decir, tendremos 128 filtros convolucionales con un kernel de $9\\times 9$, en la segunda capa tendremos 64 filtros y un kernel de $5\\times 5$ y finalmente en la última capa, únicamente tendra un filtro convolucional y el tamaño de kernel será de $5\\times 5$. \n",
    "La idea de tener 3 filtros se puede encontrar en https://arxiv.org/abs/1501.00092 , en donde se indica que los métodos tradicionales de super resolución generalmente se basan en 3 pasos, los cuales son:\n",
    "1. Extracción de parche y representación\n",
    "2. Mapeo no lineal\n",
    "3. Reconstrucción\n",
    "\n",
    "De esta forma, se pueden tener estas 3 etapas en una única red convolucional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a600a28-97fd-4e6d-9d87-f420e92ca819",
   "metadata": {},
   "source": [
    "## Importar librerias\n",
    "Antes de comenzar, importaremos las librerias de las cuales estaremos haciendo uso durante el entrenamiento de la red neuroanl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf66e6-d314-4924-81ef-16e2b080fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import keras\n",
    "import cv2\n",
    "import numpy\n",
    "import matplotlib\n",
    "import skimage\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('Keras: {}'.format(keras.__version__))\n",
    "print('OpenCV: {}'.format(cv2.__version__))\n",
    "print('NumPy: {}'.format(numpy.__version__))\n",
    "print('Matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('Scikit-Image: {}'.format(skimage.__version__))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import adam_v2\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import keras.preprocessing.image_dataset as kds\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "# python magic function, displays pyplot figures in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# List of the GPU available of the machine, this is for use the GPU NVIDIA GeForce 940MX to train the model\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "if(physical_devices[0].device_type != None):\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(physical_devices[0])\n",
    "\n",
    "# Indicate the text size of the matplotlib to plot the history results of the model train\n",
    "fontText = {'family' : 'Times New Roman',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 28}\n",
    "fontTitle = {'family' : 'Arial',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 36}\n",
    "matplotlib.rc('font', **fontText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546d471-cc95-457b-97b2-5c95dc904a32",
   "metadata": {},
   "source": [
    "## 1. Generar imagenes en baja resolución\n",
    "Para el entrenamiento de nuestra red, a partir de nuestra **dataset** de entrenamiento en alta resolución, generaremos las imagenes en baja resolución, de manera que obtendremos dos conjuntos de imagenes, uno en baja resolución (Obtenido a partir del conjunto original) y el conjunto de entrenamiento original (Alta resolución)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20c4a1-411c-4f35-b34e-4df9cad9cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 4                                                      # Factor de escalamiento para degradado de imagen original\n",
    "Folder = 'Factor_{}_RGB'.format(factor)                         # Directorio en donde se almacenaran los resultados\n",
    "\n",
    "if not os.path.exists(Folder):\n",
    "    os.makedirs(Folder)\n",
    "if not os.path.exists(Folder + '/Training_Process'):\n",
    "    os.makedirs(Folder + '/Training_Process')\n",
    "\n",
    "# Directorios de donde se obtendran el dataset de entrenamiento (Baja resolución) y el dataset \"Target\" (Alta resolución)\n",
    "pathHR = 'TrainHR'\n",
    "pathLR = Folder + '/TrainLR'\n",
    "paths = os.listdir(pathHR)\n",
    "if not os.path.exists(pathLR):\n",
    "    os.makedirs(pathLR)\n",
    "    \n",
    "# Generar a partir de las imagenes en baja resolución las imagenes en alta resolución\n",
    "for file in paths:\n",
    "    # Abrir el archivo\n",
    "    img = cv2.imread(pathHR + '/' + file)\n",
    "\n",
    "    # Determinar las nuevas dimensiones a partir de las dimensiones originales, dependiendo el factor de degradado\n",
    "    h, w, _ = img.shape\n",
    "    new_height = int(h / factor)\n",
    "    new_width = int(w / factor)\n",
    "\n",
    "    # Reescalar la imagen - Reducir\n",
    "    img = cv2.resize(img, (new_width , new_height), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    # Reescalar la imagen - Ampliar\n",
    "    img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    # Guardar la imagen\n",
    "    cv2.imwrite('{}/{}'.format(pathLR,file), img)\n",
    "\n",
    "print('Todas las imagenes en baja resolución han sido generadas')\n",
    "pathsLR = os.listdir(pathLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa8c93-d900-41f1-98be-ce776177a85b",
   "metadata": {},
   "source": [
    "## 2. Preparar conjuntos de entrenamiento y prueba\n",
    "Cuando se realiza entrenamiento a una red neuronal, es necesario que, dentro del conjunto de entrenamiento, contar con un conjunto para pruebas, esto con el fin de \"validar\" durante el proceso de entrenamiento como de bien se va efectuando el aprendizaje por parte de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6801c0-ad2d-44ab-a6f0-bc78b70c7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizar el 10% del dataset de entrenamiento como dataset de pruebas\n",
    "n = len(pathsLR)\n",
    "trainN = round(n * .9)\n",
    "\n",
    "# Generar aleatoreidad en el dataset\n",
    "randomPaths = np.copy(pathsLR)\n",
    "np.random.shuffle(randomPaths)\n",
    "\n",
    "trainPaths = randomPaths[:]\n",
    "testPaths = randomPaths[trainN:n]\n",
    "\n",
    "print('Dataset : {} images'.format(len(paths)))\n",
    "print('test : {} images'.format(len(testPaths)))\n",
    "print('train : {} images'.format(len(trainPaths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbe3de-b3fc-4343-9459-2bee7772529c",
   "metadata": {},
   "source": [
    "## 3. Pre-procesamiento (Callback map) para generar el dataset de entrenamiento\n",
    "Se asume que se reciben imagenes en $RGB$. Para el entrenamiento de nuestra red, lo realizaremos a través de los 3 canales del espacio de color $RGB$. Para ello primero leemos la imagen en $RGB$, despues realizamos una división entre $255$ a esta imagen transformada (Normalizamos la imagen), esto con el fin de reducir la variación de los valores en nuestra imagen y por lo tanto reducir también la variación en los parámetros de nuestra red neuronal. Para reconstruir la imagen debemos multiplicar la imagen de salida por $255$.\n",
    "\n",
    "$$Entrada=\\frac{RGB}{255}$$\n",
    "$$Salida=(SalidaSRCNN\\times 255)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04984b43-9d3e-4824-87d2-0ca698fd2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadImage(file):\n",
    "    Input,Target = tf.py_function(LoadImageNumpy, [file], [np.float32, np.float32])\n",
    "    return Input,Target\n",
    "\n",
    "def LoadImageNumpy(filepath):\n",
    "    if(isinstance(filepath, tf.Tensor)):\n",
    "        file = np.array(tnp.array(filepath)).astype(str)\n",
    "    else:\n",
    "        file = filepath\n",
    "    Input = cv2.imread('{}/{}'.format(pathLR, file))\n",
    "    Target = cv2.imread('{}/{}'.format(pathHR, file))\n",
    "    Input = cv2.cvtColor(Input, cv2.COLOR_BGR2RGB).astype(np.float32)/255\n",
    "    Target = cv2.cvtColor(Target, cv2.COLOR_BGR2RGB).astype(np.float32)/255\n",
    "    return Input,Target\n",
    "\n",
    "imgLR,imgHR = LoadImage(paths[10])\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Baja resolución')\n",
    "plt.imshow(imgLR)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Alta resolución')\n",
    "plt.imshow(imgHR)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37216a4-5c04-4699-8917-75f6575c60ba",
   "metadata": {},
   "source": [
    "## 4. Generar Dataset\n",
    "Se generarán las variables **dataset** para el entrenamiento y validación de la red. Este objeto debe devolver la imagen en baja resolución así como su respectivo par en alta resolución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e245e-3864-4b05-9a19-253ea6a78d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el dataset mediante la libereria tensorflow\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(trainPaths)\n",
    "ds_train = ds_train.map(LoadImage, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.batch(1)\n",
    "\n",
    "ds_test = tf.data.Dataset.from_tensor_slices(testPaths)\n",
    "ds_test = ds_test.map(LoadImage, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(1)\n",
    "\n",
    "for imgLR,imgHR in ds_train.take(1):\n",
    "    pass\n",
    "\n",
    "# Verificar que el dataset devuelve la imagen en baja resolución y su respectiva imagen en alta resolución\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Baja resolución')\n",
    "plt.imshow(imgLR[0,:,:,:])\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Alta resolución')\n",
    "plt.imshow(imgHR[0,:,:,:])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ae72e-be48-4436-b1a3-25269e723157",
   "metadata": {},
   "source": [
    "## 5. Generar modelo de la red\n",
    " Se generará la red neruoanal convolucional, la cual estará constituida por 3 capas convolucionales como se menciono anteriormente. y una vez generada, se dará comienzo al entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16b08f0-0b75-4a2c-9629-a321bb6f0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model type\n",
    "def model():\n",
    "    SRCNN = Sequential()\n",
    "\n",
    "    # Agregar las capas del modelo, en orden secuencial\n",
    "    SRCNN.add(Conv2D(filters=128, kernel_size = (9, 9), kernel_initializer='glorot_uniform',\n",
    "                     activation='relu', padding='same', use_bias=True, strides = 1, input_shape=(None, None, 3)))\n",
    "    SRCNN.add(Conv2D(filters=64, kernel_size = (5, 5), kernel_initializer='glorot_uniform',\n",
    "                     activation='relu', padding='same', use_bias=True))\n",
    "    SRCNN.add(Conv2D(filters=3, kernel_size = (5, 5), kernel_initializer='glorot_uniform',\n",
    "                     activation='linear', padding='same', use_bias=True))\n",
    "    \n",
    "    # Definir optimizador del modelo, para este caso utilizaremos el optimizados Adam, para capas 1 y 2 lr=3e-3\n",
    "    # y para la capa 3 lr=3e-4\n",
    "    adam1 = adam_v2.Adam(learning_rate=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-9)\n",
    "    adam2 = adam_v2.Adam(learning_rate=0.0003, beta_1=0.9, beta_2=0.999, epsilon=1e-9)\n",
    "    optimizers_and_layers = [(adam1, SRCNN.layers[:1]), (adam2, SRCNN.layers[2])]\n",
    "    adam = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return SRCNN\n",
    "\n",
    "SRCNN = model()\n",
    "print(SRCNN.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be896548-94c2-45a5-8b2f-6161f6aa895f",
   "metadata": {},
   "source": [
    "## 6. Verificar predicción de la red\n",
    "Se puede cargar un modelo pre-entrenado desde la carpeta \"Models\" o bien utilizar la red que esta siendo entrenada en este Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f17d9-6374-4c6b-943f-3531c0fc1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de almacenamiento de entrenamiento de la red\n",
    "if not os.path.exists(Folder + '/models'):\n",
    "    os.makedirs(Folder + '/models')\n",
    "\n",
    "#SRCNN.load_weights(Folder + '/models/model_250epochs_3c.h5')                   # Cargar modelo pre-entrenado\n",
    "\n",
    "# Cargar imagenes de baja resolución y de alta resolución respectivamente, esto con el fin de comparar los resultados\n",
    "tmpImgHR = cv2.imread(pathHR + '/t60.bmp')\n",
    "tmpImgHR = cv2.cvtColor(tmpImgHR, cv2.COLOR_BGR2RGB)\n",
    "tmpImgLR = cv2.imread(pathLR + '/t60.bmp')\n",
    "tmpImgLR = cv2.cvtColor(tmpImgLR, cv2.COLOR_BGR2RGB)\n",
    "tmpImg = tmpImgLR.astype(np.float32)/255\n",
    "dim = tmpImg.shape\n",
    "\n",
    "# Preparar entrada para predicción por la red\n",
    "Input_train = np.zeros((1,dim[0],dim[1],dim[2]))\n",
    "Input_train[0,:,:,:] = tmpImg[:,:,:]\n",
    "\n",
    "# Comprobar funcionamiento de la red convolucional creada\n",
    "salida_test = SRCNN.predict(Input_train, batch_size = 1)\n",
    "\n",
    "# Pos-procesamiento del resultado de salida de la red\n",
    "tmp = salida_test[0,:,:,:]\n",
    "\n",
    "for index,x in np.ndenumerate(tmp):\n",
    "    if tmp[index] < 0:\n",
    "        tmp[index] = 0 #-tmp[index]\n",
    "    if tmp[index] > 1:\n",
    "        tmp[index] = 1 # - tmp[index]\n",
    "\n",
    "# Dar formato a imagen RGB con valores de [0,255]\n",
    "tmp = (tmp*255).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title('Alta Resolución')\n",
    "plt.imshow(tmpImgHR)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('SRCNN')\n",
    "plt.imshow(tmp)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Baja Resolución')\n",
    "plt.imshow(tmpImgLR)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d68db-e2fe-4d74-9315-f5fb02f96339",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Pre-procesamiento de la imagen y Callbacks para monitoreo de entrenamiento de la red\n",
    "- Como se menciono anteriormente, se debe realizar la división entre $255$ para reducir la variación de parámetros.\n",
    "- Para el entrenamiento de la red, le pasaremos como _Input_ la imagen \"normalizada\" que acabamos de transformar en el paso anterior, de manera que el entrenamiento de la red será realizado en los 3 canales RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0dd2d-1035-4e0d-a0c8-1fe5c040ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpImg = cv2.imread(pathLR + '/t60.bmp')\n",
    "tmpImg = cv2.cvtColor(tmpImg, cv2.COLOR_BGR2RGB).astype(np.float32)/255\n",
    "dim = tmpImg.shape\n",
    "Input_train = np.zeros((1,dim[0],dim[1],dim[2]))\n",
    "Input_train[0,:,:,:] = tmpImg[:,:,:]\n",
    "\n",
    "def TrainObservation(SRCNN,epoch,epoch_start,Input,tmpImg,ResPath):\n",
    "    output = SRCNN.predict(Input, batch_size=1)\n",
    "    \n",
    "    tmp = output[0,:,:,:]\n",
    "    \n",
    "    # Pre-procesar la imagen (Metodo 2)\n",
    "    for index,x in np.ndenumerate(tmp):\n",
    "        if tmp[index] < 0:\n",
    "            tmp[index] = 0 #-tmp[index]\n",
    "        if tmp[index] > 1:\n",
    "            tmp[index] = 1 #2 - tmp[index]\n",
    "    \n",
    "    tmp = cv2.cvtColor((tmp*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite('{}/{}/{}.png'.format(Folder, ResPath, epoch_start + epoch),tmp)\n",
    "\n",
    "class TrainCallbacks(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (np.mod(epoch,10) == 0 and epoch != 0 ):\n",
    "            TrainObservation(self.model,epoch,start_epoch,Input_train,tmpImg,'Training_Process')\n",
    "            self.model.save('{}/models/model_{}epochs_3c.h5'.format(Folder, start_epoch + epoch), include_optimizer = False)\n",
    "        if(epoch < 150):\n",
    "            adam1 = adam_v2.Adam(learning_rate=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-9)\n",
    "            adam2 = adam_v2.Adam(learning_rate=0.0003, beta_1=0.9, beta_2=0.999, epsilon=1e-9)\n",
    "            optimizers_and_layers = [(adam1, SRCNN.layers[:1]), (adam2, SRCNN.layers[2])]\n",
    "            adam = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "        elif(epoch >= 150):\n",
    "            adam1 = adam_v2.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-9)\n",
    "            adam2 = adam_v2.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-9)\n",
    "            optimizers_and_layers = [(adam1, SRCNN.layers[:1]), (adam2, SRCNN.layers[2])]\n",
    "            adam = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9932e0-6579-4584-bf56-0d0844e9853b",
   "metadata": {},
   "source": [
    "## 8. Comenzar entrenamiento de la red\n",
    "Se dara comienzo al entrenamiento de la red, para esto, se define a partir de cual \"epoca\" inicial comenzará el entrenamiento (Esto es útil cuando deseamos continuar el entrenamiento a partir de un modelo previamente entrenado), la cantidad de epocas que se entrenará al modelo y la frecuencia con la que se ejecutara el dataset de pruebas (Verificación con una porción del dataset de imagenes el entrenamiento de la red cuando está esta siendo entrenada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac913de-0bb4-4415-87e2-eae7440c5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de epocas que se entrenara la red\n",
    "start_epoch = 0\n",
    "train_epoch = 30\n",
    "val_freq = 5\n",
    "\n",
    "# Definir optimizador del modelo, para este caso utilizaremos el optimizados Adam, para capas 1 y 2 lr=1e-3\n",
    "# y para la capa 3 lr=1e-4\n",
    "adam1 = adam_v2.Adam(learning_rate=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-9)\n",
    "adam2 = adam_v2.Adam(learning_rate=0.0003, beta_1=0.9, beta_2=0.999, epsilon=1e-9)\n",
    "optimizers_and_layers = [(adam1, SRCNN.layers[:1]), (adam2, SRCNN.layers[2])]\n",
    "adam = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "\n",
    "# Compilar el modelo\n",
    "SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Cargar un modelo previamente entrenado, a fin de continuar el entrenamiento a partir de este modelo previo\n",
    "if(os.path.exists('{}/models/model_{}epochs_3c.h5'.format(Folder,start_epoch))):\n",
    "    SRCNN.load_weights('{}/models/model_{}epochs_3c.h5'.format(Folder,start_epoch))\n",
    "else:\n",
    "    print('No existe un modelo correspondiente a la epoca inicial : {}'.format(start_epoch))\n",
    "\n",
    "if(train_epoch > 0):\n",
    "    # Comenzar entrenamiento de la red\n",
    "    history = SRCNN.fit(ds_train, validation_data=ds_test, epochs=train_epoch, validation_freq=val_freq, callbacks=[TrainCallbacks()])\n",
    "\n",
    "    # Predecir resultado de imagen de entrada y guardar el resultado\n",
    "    TrainObservation(SRCNN,train_epoch,start_epoch,Input_train,tmpImg,'Training_Process')\n",
    "\n",
    "    # Guardar el modelo entrenado para su posterior utilización\n",
    "    SRCNN.save('{}/models/model_{}epochs_3c.h5'.format(Folder, start_epoch + train_epoch), include_optimizer = False)\n",
    "\n",
    "    # Graficar los resultados del entrenamiento de la red\n",
    "    plt.figure(figsize=(30,20))\n",
    "    #plt.plot(history.history['loss'], label='Loss (training data)')\n",
    "    plt.plot(history.history['mean_squared_error'], label='MSE (training data)')\n",
    "    #plt.plot(history.history['val_loss'], label='Loss (validation data)')\n",
    "    plt.title('Training SRCNN results',fontdict=fontTitle)\n",
    "    plt.xlabel('Epoch',fontdict=fontTitle)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig('{}/TrainingHistory_{}-{}_epochs.png'.format(Folder, start_epoch,start_epoch + train_epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1824db-3baf-4f44-9818-7efc01ce36c4",
   "metadata": {},
   "source": [
    "## 9. Verificar resultados obtenidos de la red entrenada\n",
    "Pasaremos de manera iterativa una imagen de prueba, es decir, definiremos cuantas veces la salida de la red convolucional será nuevamente la entrada de está misma. Es decir, inicialmente pasamos la imagen de prueba en baja resolución, despues el resultado obtenido lo pasaremos como entrada a la red, y asi iterativamente hasta obtener un resultado \"mejorado\" de la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9ff09-c62c-48e9-bbfa-835c5367ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadTestImg(path):\n",
    "    # Cargamos la imagen de prueba\n",
    "    tmpImg = cv2.imread(path)\n",
    "    tmpImg = cv2.cvtColor(tmpImg, cv2.COLOR_BGR2RGB).astype(np.float32)/255\n",
    "    dim = tmpImg.shape\n",
    "    Input_train = np.zeros((1,dim[0],dim[1],dim[2]))\n",
    "    Input_train[0,:,:,:] = tmpImg[:,:,:]\n",
    "    return tmpImg,Input_train\n",
    "\n",
    "# Generamos la red convolucional\n",
    "SRCNN = model()\n",
    "\n",
    "# Epoca de entrenamiento de la red con la cual vamos a comprobar el resultado\n",
    "epoch_model = 350\n",
    "iterations = 10\n",
    "\n",
    "# Especificar directorios\n",
    "testFile = pathLR + '/t60.bmp'\n",
    "if not(os.path.exists(Folder + '/Test')):\n",
    "    os.makedirs(Folder + '/Test')\n",
    "\n",
    "# Cargar el modelo previamente entrenado, a fin de realizar la comprobación de resultados con este modelo entrenado\n",
    "if(os.path.exists('{}/models/model_{}epochs_3c.h5'.format(Folder,epoch_model))):\n",
    "    SRCNN.load_weights('{}/models/model_{}epochs_3c.h5'.format(Folder,epoch_model))\n",
    "    tmpImg = cv2.imread(testFile)\n",
    "    cv2.imwrite('{}/Test/0.png'.format(Folder),tmpImg)\n",
    "    tmpImg,Input_train = LoadTestImg(testFile)\n",
    "    for i in range(iterations):\n",
    "        TrainObservation(SRCNN, i + 1, 0, Input_train, tmpImg, 'Test')\n",
    "        print(i + 1)\n",
    "        tmpImg,Input_train = LoadTestImg('{}/Test/{}.png'.format(Folder,i+1))\n",
    "    print('Tarea terminada')\n",
    "else:\n",
    "    print('No existe un modelo correspondiente a la epoca inicial : {}'.format(epoch_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
